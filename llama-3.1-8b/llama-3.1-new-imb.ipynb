{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import bitsandbytes as bnb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(69420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=pd.read_csv(\"../final_dataset/train_set.csv\")\n",
    "df1=pd.read_csv(\"../final_dataset/imbalanced_train_set.csv\")\n",
    "df2=pd.read_csv(\"../final_dataset/eval_set.csv\")\n",
    "df3=pd.read_csv(\"../final_dataset/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= df1[['Contents','Secret','Label']]\n",
    "print(df1['Label'].value_counts())\n",
    "\n",
    "df2= df2[['Contents','Secret','Label']]\n",
    "print(df2['Label'].value_counts())\n",
    "\n",
    "df3= df3[['Contents','Secret','Label']]\n",
    "print(df3['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Label'] = df1['Label'].replace({0: 'Non-sensitive', 1: 'Secret'})\n",
    "print(df1['Label'].value_counts())\n",
    "\n",
    "df2['Label'] = df2['Label'].replace({0: 'Non-sensitive', 1: 'Secret'})\n",
    "print(df2['Label'].value_counts())\n",
    "\n",
    "df3['Label'] = df3['Label'].replace({0: 'Non-sensitive', 1: 'Secret'})\n",
    "print(df3['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_window(text, target_string, window_size=200):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n",
    "\n",
    "df1['Contents'] = df1.apply(lambda row: create_context_window(row['Contents'], row['Secret']), axis=1)\n",
    "df2['Contents'] = df2.apply(lambda row: create_context_window(row['Contents'], row['Secret']), axis=1)\n",
    "df3['Contents'] = df3.apply(lambda row: create_context_window(row['Contents'], row['Secret']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df1\n",
    "X_eval = df2\n",
    "X_test = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "You are a code security auditor or classifier speccialized in identifying and categorizing sensitive secrets from code snippet.Classify the given candidate string as either \"Non-sensitive\" or \"Secret\" based on its role in the provided code snippet. A \"Secret\" includes sensitive information such as: API keys and secrets (e.g., `sk_test_ABC123`), Private and secret keys (e.g., private SSH keys, private cryptographic keys), Authentication keys and tokens (e.g., `Bearer <token>`), Database connection strings with credentials (e.g., `mongodb://user:password@host:port`), Passwords, usernames, and any other private information that should not be shared openly. A \"Non-sensitive\" string is not considered secret and can be shared openly. This may include: Publicly available keys (e.g., public SSH keys), Non-sensitive configuration values or identifiers, Any non-sensitive data not directly tied to security or authentication. Carefully consider the context of the string in the provided code. If the string is part of authentication, encryption, or access control, it is likely a \"Secret\". Otherwise, it is \"Non-sensitive\". Ensure you pay attention to specific patterns like tokens, passwords, or keys in the string. Return the answer as the corresponding label.\n",
    "\n",
    "candidate_string: {data_point[\"Secret\"]}\n",
    "code_snippet: {data_point[\"Contents\"]}\n",
    "label: {data_point[\"Label\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "You are a code security auditor or classifier speccialized in identifying and categorizing sensitive secrets from code snippet.Classify the given candidate string as either \"Non-sensitive\" or \"Secret\" based on its role in the provided code snippet. A \"Secret\" includes sensitive information such as: API keys and secrets (e.g., `sk_test_ABC123`), Private and secret keys (e.g., private SSH keys, private cryptographic keys), Authentication keys and tokens (e.g., `Bearer <token>`), Database connection strings with credentials (e.g., `mongodb://user:password@host:port`), Passwords, usernames, and any other private information that should not be shared openly. A \"Non-sensitive\" string is not considered secret and can be shared openly. This may include: Publicly available keys (e.g., public SSH keys), Non-sensitive configuration values or identifiers, Any non-sensitive data not directly tied to security or authentication. Carefully consider the context of the string in the provided code. If the string is part of authentication, encryption, or access control, it is likely a \"Secret\". Otherwise, it is \"Non-sensitive\". Ensure you pay attention to specific patterns like tokens, passwords, or keys in the string. Return the answer as the corresponding label.\n",
    "\n",
    "candidate_string: {data_point[\"Secret\"]}\n",
    "code_snippet: {data_point[\"Contents\"]}\n",
    "label: \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompts for training and evaluation data\n",
    "X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
    "X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n",
    "\n",
    "# Generate test prompts and extract true labels\n",
    "y_true = X_test.loc[:,'Label']\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datasets\n",
    "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
    "eval_data = Dataset.from_pandas(X_eval[[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('a.txt', 'w') as f:\n",
    "    f.write(train_data['text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens=2, \n",
    "                        temperature=0.1)\n",
    "        \n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n",
    "        \n",
    "        # Determine the predicted label (Secret or Non-sensitive)\n",
    "        if \"Secret\" in answer:\n",
    "            y_pred.append(\"Secret\")\n",
    "        else:\n",
    "            y_pred.append(\"Non-sensitive\")\n",
    "        \n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(X_test, model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    def map_func(x):\n",
    "        if x == \"Non-sensitive\":\n",
    "            return 0\n",
    "        elif x == \"Secret\":\n",
    "            return 1\n",
    "        else:\n",
    "            return -1  # Handle unexpected labels (optional)\n",
    "\n",
    "    # Map the true and predicted labels to integers\n",
    "    y_true_mapped = np.array([map_func(label) for label in y_true])\n",
    "    y_pred_mapped = np.array([map_func(label) for label in y_pred])\n",
    "\n",
    "    # Filter out invalid labels (-1)\n",
    "    valid_indices = np.where((y_true_mapped != -1) & (y_pred_mapped != -1))[0]\n",
    "    y_true_mapped = y_true_mapped[valid_indices]\n",
    "    y_pred_mapped = y_pred_mapped[valid_indices]\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Overall Accuracy: {accuracy:.3f}')\n",
    "\n",
    "    # Calculate accuracy for each label\n",
    "    labels = [0, 1]\n",
    "    label_names = [\"Non-sensitive\", \"Secret\"]\n",
    "    for label, name in zip(labels, label_names):\n",
    "        label_indices = np.where(y_true_mapped == label)[0]\n",
    "        label_accuracy = accuracy_score(\n",
    "            y_true=y_true_mapped[label_indices], \n",
    "            y_pred=y_pred_mapped[label_indices]\n",
    "        ) if len(label_indices) > 0 else 0.0\n",
    "        print(f'Accuracy for {name}: {label_accuracy:.3f}')\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(\n",
    "        y_true=y_true_mapped, \n",
    "        y_pred=y_pred_mapped, \n",
    "        target_names=label_names, \n",
    "        labels=labels,\n",
    "        digits=4\n",
    "    )\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(\n",
    "        y_true=y_true_mapped, \n",
    "        y_pred=y_pred_mapped, \n",
    "        labels=labels\n",
    "    )\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "evaluate(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Get the most likely token prediction for each position\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Calculate accuracy only on non-padded tokens\n",
    "    # Assuming pad_token_id is tokenizer.pad_token_id\n",
    "    mask = labels != -100  # Ignore padding tokens\n",
    "    accuracy = (predictions[mask] == labels[mask]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"../models/llama-fine-tuned-model-30k-new-prompt-1024-imb\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules,\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=7,                       # number of training epochs\n",
    "    per_device_train_batch_size=1,            # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # Add this to reduce eval memory usage\n",
    "    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=100000, \n",
    "    logging_strategy=\"epoch\",                        \n",
    "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler                  # report metrics to w&b\n",
    "    report_to=[\"none\"],\n",
    "    save_strategy=\"epoch\",           # Change from eval_strategy=\"steps\" to save_strategy=\"epoch\"\n",
    "    save_total_limit=1,             # Optional: keep only the last 1 checkpoints to save disk space\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps = 0.2,\n",
    "    disable_tqdm=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    "    \n",
    "    dataset_kwargs={\n",
    "    \"add_special_tokens\": False,\n",
    "    \"append_concat_token\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom training loop with tqdm\n",
    "# train_dataloader = trainer.get_train_dataloader()\n",
    "# total_batches = len(train_dataloader) * training_arguments.num_train_epochs\n",
    "\n",
    "# # Create a tqdm progress bar\n",
    "# with tqdm(total=total_batches, desc=\"Training Progress\") as pbar:\n",
    "#     for epoch in range(int(training_arguments.num_train_epochs)):\n",
    "#         for step, batch in enumerate(train_dataloader):\n",
    "#             # Perform a training step\n",
    "#             trainer.training_step(model, batch)\n",
    "#             pbar.update(1)\n",
    "trainer.train()\n",
    "# checkpoint_path = \"llama-3.1-fine-tuned-model-20k/checkpoint-12000\"\n",
    "\n",
    "# trainer.train(resume_from_checkpoint=checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# Define the filename and extract the base name (without path and extension)\n",
    "filename = '../plots/llama-30k-7e-new-prompt-1024-imb.png'\n",
    "base_filename = os.path.splitext(os.path.basename(filename))[0]\n",
    "\n",
    "# Create a heatmap from the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])\n",
    "\n",
    "# Set the title dynamically to match the filename\n",
    "plt.title(f\"Confusion Matrix: {base_filename}\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "\n",
    "# Save and display the plot\n",
    "plt.savefig(filename)  # Save the plot as PNG file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt = f\"\"\"\n",
    "            Classify the given candidate string into \"Non-sensitive\" or \"Secret\" based on its presence and usage in the provided code snippet. A \"Secret\" refers to sensitive information like API keys, passwords, or private tokens. Return the answer as the corresponding label.\n",
    "candidate_string: \"sk_test_4eC39HqLyjWDarjtT1zdp7dc\"\n",
    "code snippet: \n",
    "import requests\n",
    "\n",
    "API_KEY = \"sk_test_4eC39HqLyjWDarjtT1zdp7dc\"  # Secret\n",
    "\n",
    "response = requests.get(f\"https://api.stripe.com/v1/charges\", headers={{\n",
    "    \"Authorization\": f\"Bearer API_KEY\"\n",
    "}})\n",
    "print(response.json())\n",
    "label: \"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=2, do_sample=True, temperature=0.1)\n",
    "print(outputs[0][\"generated_text\"].split(\"label: \")[-1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secretbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
