{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import bitsandbytes as bnb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(69420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3=pd.read_csv(\"../multiclass_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= df1[['Contents','Secret','Label']]\n",
    "# print(df1['Label'].value_counts())\n",
    "\n",
    "# df2= df2[['Contents','Secret','Label']]\n",
    "# print(df2['Label'].value_counts())\n",
    "\n",
    "# df3= df3[['Contents','Secret','Label']]\n",
    "# print(df3['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df3['Label'] = df3['Label'].replace({0: 'Non-sensitive', 1: 'Secret'})\n",
    "print(df3['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_window(text, target_string, window_size=200):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n",
    "\n",
    "df3['Contents'] = df3.apply(lambda row: create_context_window(row['Contents'], row['Secret']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            You are a code security auditor or classifier speccialized in identifying and categorizing sensitive secrets from code snippet.Classify the given candidate string into one of the following categories based on its presence and usage in the provided code snippet:\n",
    "            \n",
    "            - Private Key  \n",
    "            - API Key and Secret  \n",
    "            - Authentication Key and Token  \n",
    "            - Other  \n",
    "            - Generic Secret  \n",
    "            - Database and Server URL  \n",
    "            - Password  \n",
    "            - Username  \n",
    "\n",
    "            A secret refers to sensitive information like API keys, passwords, private tokens, and credentials. Analyze the candidate string in the given code snippet and determine its correct category.\n",
    "\n",
    "candidate_string: {data_point[\"Secret\"]}\n",
    "code snippet: {data_point[\"Contents\"]}\n",
    "label: {data_point[\"Category\"]}\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            You are a code security auditor or classifier speccialized in identifying and categorizing sensitive secrets from code snippet.Classify the given candidate string into one of the following categories based on its presence and usage in the provided code snippet:\n",
    "            \n",
    "            - Private Key  \n",
    "            - API Key and Secret  \n",
    "            - Authentication Key and Token  \n",
    "            - Other  \n",
    "            - Generic Secret  \n",
    "            - Database and Server URL  \n",
    "            - Password  \n",
    "            - Username  \n",
    "\n",
    "            A secret refers to sensitive information like API keys, passwords, private tokens, and credentials. Analyze the candidate string in the given code snippet and determine its correct category.\n",
    "\n",
    "candidate_string: {data_point[\"Secret\"]}\n",
    "code snippet: {data_point[\"Contents\"]}\n",
    "label: \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate test prompts and extract true labels\n",
    "y_true = X_test.loc[:,'Category']\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # base_model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# checkpoint_path = \"../models/deepseek-12k-7e-multi/checkpoint-7938\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16  # Use float16 for faster computation\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     checkpoint_path, \n",
    "#     quantization_config=bnb_config, \n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1\n",
    "\n",
    "# # Apply LoRA for memory-efficient fine-tuning\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,  # Low-rank adaptation size\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\"\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, use_fast=False)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "# Base model & tokenizer\n",
    "base_model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "checkpoint_path = \"../models/deepseek-12k-7e-multi/checkpoint-7938\"\n",
    "\n",
    "# 1. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 2. Set quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 3. Load base model with quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# 4. Load fine-tuned LoRA adapter on top of base model\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "\n",
    "# Optional: merge LoRA weights into the base model if you're done with training\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    \n",
    "    categories = [\n",
    "        \"Private Key\",\n",
    "        \"API Key and Secret\",\n",
    "        \"Authentication Key and Token\",\n",
    "        \"Other\",\n",
    "        \"Generic Secret\",\n",
    "        \"Database and Server URL\",\n",
    "        \"Password\",\n",
    "        \"Username\"\n",
    "    ]\n",
    "    \n",
    "    pipe = pipeline(task=\"text-generation\", \n",
    "                    model=model, \n",
    "                    tokenizer=tokenizer, \n",
    "                    max_new_tokens=5,  # Allow more tokens for longer labels\n",
    "                    temperature=0.1)\n",
    "    \n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        \n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n",
    "        \n",
    "        # Ensure the answer is a valid category\n",
    "        predicted_label = next((cat for cat in categories if cat in answer), \"Other\")  \n",
    "        y_pred.append(predicted_label)\n",
    "        \n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    # Define category mappings\n",
    "    categories = [\n",
    "        \"Private Key\",\n",
    "        \"API Key and Secret\",\n",
    "        \"Authentication Key and Token\",\n",
    "        \"Other\",\n",
    "        \"Generic Secret\",\n",
    "        \"Database and Server URL\",\n",
    "        \"Password\",\n",
    "        \"Username\"\n",
    "    ]\n",
    "    category_map = {category: i for i, category in enumerate(categories)}\n",
    "\n",
    "    # Map string labels to integer values\n",
    "    y_true_mapped = np.array([category_map[label] for label in y_true])\n",
    "    y_pred_mapped = np.array([category_map[label] for label in y_pred])\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Overall Accuracy: {accuracy:.3f}')\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(\n",
    "        y_true=y_true_mapped, \n",
    "        y_pred=y_pred_mapped, \n",
    "        target_names=categories, \n",
    "        digits=4\n",
    "    )\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(\n",
    "        y_true=y_true_mapped, \n",
    "        y_pred=y_pred_mapped\n",
    "    )\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "evaluate(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each category in y_pred\n",
    "y_pred_counts = Counter(y_pred)\n",
    "\n",
    "# Print results\n",
    "for category, count in y_pred_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "y_true_counts = Counter(y_true)\n",
    "\n",
    "# Print results\n",
    "for category, count in y_true_counts.items():\n",
    "    print(f\"{category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    # Define category mappings\n",
    "    categories = [\n",
    "        \"Private Key\",\n",
    "        \"API Key and Secret\",\n",
    "        \"Authentication Key and Token\",\n",
    "        \"Other\",\n",
    "        \"Generic Secret\",\n",
    "        \"Database and Server URL\",\n",
    "        \"Password\",\n",
    "        \"Username\"\n",
    "    ]\n",
    "    category_map = {category: i for i, category in enumerate(categories)}\n",
    "\n",
    "    # Map string labels to integer values\n",
    "    y_true_mapped = np.array([category_map[label] for label in y_true])\n",
    "    y_pred_mapped = np.array([category_map[label] for label in y_pred])\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_mapped, y_pred_mapped)\n",
    "\n",
    "     # Generate confusion matrix heatmap with correct labels\n",
    "    filename = '../plots/deepseek-12k-7e-multi.png'\n",
    "    base_filename = os.path.splitext(os.path.basename(filename))[0]\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=categories, yticklabels=categories)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix: {base_filename}\")\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = prompt = f\"\"\"\n",
    "#             Classify the given candidate string into \"Non-sensitive\" or \"Secret\" based on its presence and usage in the provided code snippet. A \"Secret\" refers to sensitive information like API keys, passwords, or private tokens. Return the answer as the corresponding label.\n",
    "# candidate_string: \"sk_test_4eC39HqLyjWDarjtT1zdp7dc\"\n",
    "# code snippet: \n",
    "# import requests\n",
    "\n",
    "# API_KEY = \"sk_test_4eC39HqLyjWDarjtT1zdp7dc\"  # Secret\n",
    "\n",
    "# response = requests.get(f\"https://api.stripe.com/v1/charges\", headers={{\n",
    "#     \"Authorization\": f\"Bearer API_KEY\"\n",
    "# }})\n",
    "# print(response.json())\n",
    "# label: \"\"\".strip()\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# outputs = pipe(prompt, max_new_tokens=2, do_sample=True, temperature=0.1)\n",
    "# print(outputs[0][\"generated_text\"].split(\"label: \")[-1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secretbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
